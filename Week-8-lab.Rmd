Week 8 Lab
=============

Our first task is to go over how to write a linear model in R. Let's take a moment to go through Logan Table 7.3.

First we will read in [some data](https://github.com/hlynch/Biometry2021/tree/master/_data/fish.txt) on mercury levels in fish. 

```{r}
data<-read.table("~/Documents/Projects/Biometry2022/_data/fish.txt",header=T)
head(data)
```

Let's use Station as a categorical variable, and use it as a covariate for a model of mercury:

```{r}
Station<-data$STATION
Mercury<-data$MERCURY
```

What's the difference between 

```{r}
Station
```

and

```{r}
as.factor(Station)
```

To see what impact this has on the modelling, let's write a basic linear model in R

```{r}
fit<-lm(Mercury~Station)
summary(fit)
```

and compare this with

```{r}
fit2<-lm(Mercury~as.factor(Station))
summary(fit2)
```

This second approach is what we are interested in, because we want to use the Stations as categorical factors. 

**<span style="color: green;">Checkpoint #1: Why is the intercept for fit2 not the same as the intercept for fit?</span>** We won't get into linear regression formally until next week, but we can see what's going on if we plot the data and the best-fit line together on the same plot.

```{r}
plot(Station, Mercury,cex=1,xlab="Station as a numeric",ylab="Mercury")
abline(a=coef(fit)[1],b=coef(fit)[2],col="red")
segments(x0=-0.5,x1=0.5,y0=coef(fit2)[1],y1=coef(fit2)[1],col="blue",lwd=4)
```

The blue segment shows you the mean mercury measurement for the first station (Station 0), whereas the intercept of the red line is a little lower. Why? Because the best fitting line for all the mercury measurements doesn't necessary pass through the mean value for the first station. 

To make our consideration of contrasts easy, let's just redefine the variable accordingly:

```{r}
Station<-as.factor(Station)
```

Let's examine this output for a second. How do we interpret the intercept? Notice that Station=0 isn't represented among the factors, it has been turned in to (a.k.a. "aliased by") the intercept. We can convince ourselves this is true using

```{r}
mean(Mercury[Station=="0"])
```

(We have to use quotes now, because the variable "Station" is a factor, which means the levels are now identified by names (characters) which must be put in quotes.)


**<span style="color: green;">Checkpoint #2: Do you understand what the intercept of fit2 means? Can you predict what the value for as.factor(Station)1 means?</span>**

We can also convince ourselves of the other model estimates using

```{r}
mean(Mercury[Station=="1"])
mean(Mercury[Station=="2"])
```


When fitting a model involving categorical factors as predictors, R will always use the first factor for the intercept, which is the one that comes first alphabetically or numerically. This may not be what you actually want to do. Why?? Because the hypotheses being tested are:

$$
H_{0}: \mbox{Factor level 1} = \mbox{Intercept}=0 
$$
$$
H_{0}: \mbox{Difference between factor level 2 and factor level 1} = 0            
$$
$$
H_{0}: \mbox{Difference between factor level 3 and factor level 1} = 0
$$
$$
H_{0}: \mbox{Difference between factor level 4 and factor level 1} = 0 
$$

etc. In other words, with the exception of the first factor which is the intercept, you are testing the equality of each factor with the first factor, not against zero (or other benchmark).

This is the "Treatment contrasts/Dummy coding" approach we discussed in lecture on Tuesday. We can confirm this by having R tell us what its default behavior is when faced with categorical predictors.

```{r}
options()$contrasts
```

(As a side note, we can just call "options()" to get all sorts of information about R's defaults.) 

We can get R to spit out the design matrix for this set of contrasts using

```{r}
contr.treatment(15)
```


Another approach would be to take the intercept out of the model, so the coefficients simply represented the group means. We can do this by including a "-1" in the model statement.


```{r}
fit3<-lm(Mercury~Station-1)
summary(fit3)
```

Notice that all stations are now treated equally, there is no intercept, and the model coefficients (the group means) are tested for significant differences from 0.

Let's pause for a second and make sure we understand how R calculated the estimates and their standard errors. The estimates are pretty straightforward, in fact we have already calculated this.

```{r}
mean(Mercury[Station=="0"])
```

But how did R calculate the standard errors? Let's stop and think for a second as to what the standard error of an estimate represents. We know the standard error of the mean of a (normal) distribution is given by

$$
\mbox{Standard error of the mean} = \frac{\mbox{standard deviation}}{\sqrt{\mbox{sample size}}} 
$$

So the standard error of any estimate is, roughly speaking, the (residual) variation divided by the square root of the sample size. (In the case of the SEM, all the variation is "residual".)

What is the residual variation in this case? One way to measure residual variation within the Station=0 group is to calculate the standard deviation of the Station=0 data.

```{r}
sd(Mercury[Station=="0"])
```

Using this as the "residual" variation, we would calculate the standard error of the coefficient for the Station=0 group as follows:

```{r}
sd(Mercury[Station=="0"])/sqrt(length(Mercury[Station=="0"]))
```

But this doesn't match what R has returned!! Why not? Because an even better estimate of the "residual" variation is the pooled estimate of the residual variation, where we use not only the residual variation within the Station=0 group, but within each of the Stations. R returned this value above: "Residual standard error:0.6274". Using this value, we calculate

```{r}
0.6274/sqrt(length(Mercury[Station=="0"]))
```

Now we get what R has returned for the standard error on the co-efficient for the Station=0 group. **<span style="color: green;">Checkpoint #3: Does this make sense?</span>** Its important to recognize that your intuition to use the standard deviation within the Station=0 group to estimate the standard error of the coefficient is not wrong, but that by pooling the data, we can get a more precise estimate of that residual standard deviation. Notice also that the only difference in standard errors between the different stations comes from the different sample sizes.

**<span style="color: green;">Checkpoint #4: Write a short script to bootstrap resample data within each station. Each of you should end up with a slightly different dataset and fitting the model to that unique dataset will yield a unique estimate for mercury at Station0. Calculate the standard deviation of your group's estimates. Is this at least approximately equal to the standard error of the estimate as reported by your model's fit?</span>**

Contrasts
--------

In lecture on Tuesday we discussed several other contrasts. Let's start with the Helmhert contrast. 

```{r}
contrasts(Station)<-"contr.helmert"
contr.helmert(15)
```

and redo the analysis (this time with an intercept)

```{r}
fit4<-lm(Mercury~Station)
summary(fit4)
```

Do these coefficients make sense? Let's work through them using the information learned in Tuesday's lecture.

The first coefficient for the Helmert contrasts is the mean of the group means. This is *not* the same as the overall mean, because different groups may have different sample sizes. We can find the mean of all the group means by using the output from fit3. Let's pull out all the coefficients from fit3, and give them a new name for convenience:

```{r}
group.means<-coef(fit3)
group.means
```

We can confirm the first Helmert parameter estimate by looking at

```{r}
mean(group.means)
```

Now the second Helmert parameter is given by

$$
\frac{\mu_{1}+\mu_{2}}{2}-\mu_{1}
$$

which we can calculate using

```{r}
((group.means[1]+group.means[2])/2)-group.means[1]
```

The third Helmert coefficient is given by

$$
\frac{\mu_{1}+\mu_{2}+\mu_{3}}{3}-\frac{\mu_{1}+\mu_{2}}{2}
$$

which we can calculate using

```{r}
((group.means[1]+group.means[2]+group.means[3])/3)-((group.means[1]+group.means[2])/2)
```

and so on.

Sum-to-zero contrasts can be done using

```{r}
contrasts(Station)<-"contr.sum"
contr.sum(15)
```

and redo the analysis (this time with an intercept)

```{r}
fit5<-lm(Mercury~Station)
summary(fit5)
```

Once again the first parameter to be estimated is the mean of the group means. The second and third parameter estimates can be confirmed as

```{r}
group.means[1]-mean(group.means)
group.means[2]-mean(group.means)
```

and so forth. **<span style="color: green;">Checkpoint #5: Do you understand how to obtain the sum-to-zero parameter estimates and what they mean in terms of this dataset?</span>**

Polynomial contrasts can be done using

```{r}
contrasts(Station)<-"contr.poly"
contr.poly(15)
```

and redo the analysis

```{r}
fit6<-lm(Mercury~Station)
summary(fit6)
```

Let's plot the contrast polynomials to get a sense for them.

```{r}
temp<-contr.poly(15)
plot(temp[,1],lwd=2,xlab="Group",ylab="Coefficient",typ="b")
lines(temp[,2],lwd=2,typ="b",col="red")
lines(temp[,3],lwd=2,typ="b",col="blue")
lines(temp[,4],lwd=2,typ="b",col="green")
lines(temp[,5],lwd=2,typ="b",col="orange")
lines(rep(1/sqrt(15),times=15),typ="b",lwd=2,col="purple")
```

where the last line plot is the intercept (a constant value whose vector magnitude equals 1) which R doesn't include in the print out of the treatments.

We can get R to plot out the model matrix for any of these models as follows:

```{r}
head(model.matrix(~Station,contrasts=list(Station="contr.treatment")),n=20)
```

or

```{r}
head(model.matrix(~Station,contrasts=list(Station="contr.helmert")),n=20)
```

